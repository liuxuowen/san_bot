"""
File analysis utilities for comparing files and generating reports
"""
import os
import re
import json
import difflib
from datetime import datetime
from typing import Tuple, Dict, Any, List

import pandas as pd


class FileAnalyzer:
    """Handles file comparison and analysis"""
    
    def __init__(self):
        self.supported_formats = ['txt', 'csv', 'json', 'xlsx', 'xls']
    
    def analyze_files(self, file1_path: str, file2_path: str, instruction: str) -> Dict[str, Any]:
        """
        Analyze and compare two files based on the given instruction
        
        Args:
            file1_path: Path to the first file
            file2_path: Path to the second file
            instruction: Instruction for comparison
            
        Returns:
            Dictionary containing analysis results
        """
        try:
            # Determine file types
            file1_ext = os.path.splitext(file1_path)[1].lower().lstrip('.')
            file2_ext = os.path.splitext(file2_path)[1].lower().lstrip('.')
            
            # Read file contents
            content1 = self._read_file(file1_path, file1_ext)
            content2 = self._read_file(file2_path, file2_ext)
            
            # Perform comparison based on instruction
            comparison_result = self._compare_contents(content1, content2, instruction)
            
            # Generate report
            report = self._generate_report(
                file1_path, file2_path, instruction, comparison_result
            )
            
            return {
                'success': True,
                'report': report,
                'details': comparison_result
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'report': f"åˆ†æå¤±è´¥: {str(e)}"
            }
    
    def _read_file(self, file_path: str, file_ext: str) -> str:
        """Read file content based on file type"""
        if file_ext in ['txt', 'csv']:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                return f.read()
        elif file_ext == 'json':
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.dumps(json.load(f), indent=2, ensure_ascii=False)
        elif file_ext in ['xlsx', 'xls']:
            try:
                import pandas as pd
                df = pd.read_excel(file_path)
                return df.to_string()
            except Exception:
                return "Excelæ–‡ä»¶è¯»å–å¤±è´¥"
        else:
            with open(file_path, 'rb') as f:
                return f.read().decode('utf-8', errors='ignore')
    
    def _compare_contents(self, content1: str, content2: str, instruction: str) -> Dict[str, Any]:
        """Compare file contents based on instruction"""
        instruction_lower = instruction.lower()
        
        # Split contents into lines for comparison
        lines1 = content1.splitlines()
        lines2 = content2.splitlines()
        
        # Calculate differences
        differ = difflib.Differ()
        diff = list(differ.compare(lines1, lines2))
        
        # Count changes
        added_lines = [line for line in diff if line.startswith('+ ')]
        removed_lines = [line for line in diff if line.startswith('- ')]
        common_lines = [line for line in diff if line.startswith('  ')]
        
        # Calculate similarity
        matcher = difflib.SequenceMatcher(None, content1, content2)
        similarity = matcher.ratio() * 100
        
        return {
            'total_lines_file1': len(lines1),
            'total_lines_file2': len(lines2),
            'added_lines': len(added_lines),
            'removed_lines': len(removed_lines),
            'common_lines': len(common_lines),
            'similarity_percentage': round(similarity, 2),
            'diff_preview': diff[:20],  # First 20 lines of diff
            'instruction': instruction
        }
    
    def _generate_report(self, file1: str, file2: str, instruction: str, 
                        comparison: Dict[str, Any]) -> str:
        """Generate a formatted report of the comparison"""
        report = f"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
æ–‡ä»¶å¯¹æ¯”åˆ†ææŠ¥å‘Š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“‹ åˆ†ææŒ‡ä»¤: {instruction}

ğŸ“ æ–‡ä»¶ä¿¡æ¯:
  æ–‡ä»¶1: {os.path.basename(file1)}
  æ–‡ä»¶2: {os.path.basename(file2)}

ğŸ“Š å¯¹æ¯”ç»“æœ:
  â€¢ æ–‡ä»¶1æ€»è¡Œæ•°: {comparison['total_lines_file1']}
  â€¢ æ–‡ä»¶2æ€»è¡Œæ•°: {comparison['total_lines_file2']}
  â€¢ ç›¸ä¼¼åº¦: {comparison['similarity_percentage']}%
  â€¢ æ–°å¢è¡Œæ•°: {comparison['added_lines']}
  â€¢ åˆ é™¤è¡Œæ•°: {comparison['removed_lines']}
  â€¢ ç›¸åŒè¡Œæ•°: {comparison['common_lines']}

ğŸ“ ç»“è®º:
"""
        
        # Generate conclusion based on similarity
        similarity = comparison['similarity_percentage']
        if similarity >= 95:
            report += "  ä¸¤ä¸ªæ–‡ä»¶å†…å®¹åŸºæœ¬ç›¸åŒï¼Œå·®å¼‚æå°ã€‚"
        elif similarity >= 80:
            report += "  ä¸¤ä¸ªæ–‡ä»¶å†…å®¹ç›¸ä¼¼åº¦è¾ƒé«˜ï¼Œå­˜åœ¨éƒ¨åˆ†å·®å¼‚ã€‚"
        elif similarity >= 50:
            report += "  ä¸¤ä¸ªæ–‡ä»¶å†…å®¹å­˜åœ¨æ˜æ˜¾å·®å¼‚ï¼Œä½†ä»æœ‰ç›¸ä¼¼ä¹‹å¤„ã€‚"
        else:
            report += "  ä¸¤ä¸ªæ–‡ä»¶å†…å®¹å·®å¼‚è¾ƒå¤§ã€‚"
        
        report += f"\n\n  æ–°å¢å†…å®¹: {comparison['added_lines']} è¡Œ"
        report += f"\n  åˆ é™¤å†…å®¹: {comparison['removed_lines']} è¡Œ"
        
        report += "\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        
        return report

    # -------------------- Custom CSV Analysis for Alliance Stats --------------------
    @staticmethod
    def _parse_cn_timestamp_from_filename(filename: str) -> datetime:
        """Parse Chinese datetime from filename like åŒç›Ÿç»Ÿè®¡YYYYå¹´MMæœˆDDæ—¥HHæ—¶MMåˆ†SSç§’.csv"""
        base = os.path.basename(filename)
        m = re.search(r"(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥(\d{1,2})æ—¶(\d{1,2})åˆ†(\d{1,2})ç§’", base)
        if not m:
            raise ValueError(f"æ— æ³•ä»æ–‡ä»¶åè§£ææ—¶é—´æˆ³: {filename}")
        y, mo, d, h, mi, s = map(int, m.groups())
        return datetime(y, mo, d, h, mi, s)

    @staticmethod
    def _read_member_stats_csv(path: str) -> pd.DataFrame:
        """Read CSV and return DataFrame with columns: æˆå‘˜, æˆ˜åŠŸæ€»é‡, åˆ†ç»„"""
        df = pd.read_csv(path, encoding='utf-8', skipinitialspace=True)
        df.columns = df.columns.str.strip()
        # Keep only needed columns, handle if some are missing
        needed = ['æˆå‘˜', 'æˆ˜åŠŸæ€»é‡', 'åˆ†ç»„']
        for col in needed:
            if col not in df.columns:
                raise ValueError(f"CSVç¼ºå°‘å¿…è¦åˆ—: {col} ({path})")
        df = df[needed].copy()
        # Normalize types
        df['æˆå‘˜'] = df['æˆå‘˜'].astype(str).str.strip()
        df['åˆ†ç»„'] = df['åˆ†ç»„'].astype(str).str.strip().replace({'': 'æœªåˆ†ç»„'})
        df['æˆ˜åŠŸæ€»é‡'] = pd.to_numeric(df['æˆ˜åŠŸæ€»é‡'], errors='coerce').fillna(0).astype(int)
        # Drop duplicate members by keeping the max æˆ˜åŠŸæ€»é‡ (defensive)
        df = df.sort_values('æˆ˜åŠŸæ€»é‡').drop_duplicates(subset=['æˆå‘˜'], keep='last').reset_index(drop=True)
        return df

    def analyze_battle_merit_change(self, file1_path: str, file2_path: str) -> Dict[str, Any]:
        """
        æ¯”å¯¹ä¸¤ä¸ªåŒç›Ÿç»Ÿè®¡CSVï¼ŒæŒ‰æ–‡ä»¶åä¸­çš„æ—¶é—´æˆ³è¯†åˆ«å…ˆåï¼Œ
        ç»Ÿè®¡åœ¨æ­¤æ—¶é—´æ®µå†…æ¯ä½æˆå‘˜çš„ æˆ˜åŠŸæ€»é‡ å·®å€¼ï¼Œå¹¶æŒ‰ åˆ†ç»„ã€å·®å€¼ æ’åºè¾“å‡ºã€‚

        Returns dict with keys: success, earlier, later, range, rows (list of dicts)
        """
        try:
            t1 = self._parse_cn_timestamp_from_filename(file1_path)
            t2 = self._parse_cn_timestamp_from_filename(file2_path)
            # Determine earlier and later
            if t1 <= t2:
                earlier_path, later_path = file1_path, file2_path
                earlier_ts, later_ts = t1, t2
            else:
                earlier_path, later_path = file2_path, file1_path
                earlier_ts, later_ts = t2, t1

            df_early = self._read_member_stats_csv(earlier_path)
            df_late = self._read_member_stats_csv(later_path)

            early = df_early.rename(columns={'æˆ˜åŠŸæ€»é‡': 'æˆ˜åŠŸæ€»é‡_æ—©', 'åˆ†ç»„': 'åˆ†ç»„_æ—©'})
            late = df_late.rename(columns={'æˆ˜åŠŸæ€»é‡': 'æˆ˜åŠŸæ€»é‡_æ™š', 'åˆ†ç»„': 'åˆ†ç»„_æ™š'})

            # Inner join: only keep members that exist in both files
            merged = pd.merge(early, late, on='æˆå‘˜', how='inner')
            # Determine group preference: later > earlier > æœªåˆ†ç»„
            merged['åˆ†ç»„'] = merged['åˆ†ç»„_æ™š'].fillna(merged['åˆ†ç»„_æ—©']).fillna('æœªåˆ†ç»„')
            merged['æˆ˜åŠŸæ€»é‡_æ—©'] = pd.to_numeric(merged['æˆ˜åŠŸæ€»é‡_æ—©'], errors='coerce').fillna(0)
            merged['æˆ˜åŠŸæ€»é‡_æ™š'] = pd.to_numeric(merged['æˆ˜åŠŸæ€»é‡_æ™š'], errors='coerce').fillna(0)
            merged['æˆ˜åŠŸæ€»é‡å·®å€¼'] = (merged['æˆ˜åŠŸæ€»é‡_æ™š'] - merged['æˆ˜åŠŸæ€»é‡_æ—©']).astype(int)

            # Build output
            result = merged[['æˆå‘˜', 'åˆ†ç»„', 'æˆ˜åŠŸæ€»é‡å·®å€¼']].copy()
            # Sort by group (asc) then diff (desc)
            result = result.sort_values(by=['åˆ†ç»„', 'æˆ˜åŠŸæ€»é‡å·®å€¼'], ascending=[True, False]).reset_index(drop=True)

            # Pack rows for return
            rows: List[Dict[str, Any]] = result.to_dict(orient='records')
            return {
                'success': True,
                'earlier': earlier_path,
                'later': later_path,
                'earlier_ts': earlier_ts.isoformat(sep=' '),
                'later_ts': later_ts.isoformat(sep=' '),
                'range': f"{earlier_ts} ~ {later_ts}",
                'rows': rows
            }
        except Exception as e:
            return {'success': False, 'error': str(e)}

    @staticmethod
    def save_grouped_tables_as_images(
        result_rows: List[Dict[str, Any]],
        out_dir: str,
        title_prefix: str,
        display_title: str,
        high_delta_threshold: int = 5000,
    ) -> List[str]:
        """Render grouped result rows as table images (one PNG per åˆ†ç»„),
        highlighting rows with æˆ˜åŠŸæ€»é‡å·®å€¼ > high_delta_threshold.
        display_title: human-readable title (dates may contain '/') used in figure, while
        title_prefix is used for filename construction (kept filesystem-safe).
        """
        import matplotlib
        matplotlib.use('Agg')
        import matplotlib.pyplot as plt
        from matplotlib.table import Table

        # Font config for Chinese
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
        plt.rcParams['axes.unicode_minus'] = False

        os.makedirs(out_dir, exist_ok=True)
        import pandas as pd
        df = pd.DataFrame(result_rows)
        saved_paths: List[str] = []
        # Prepare aggregation stats (excluding 'æœªåˆ†ç»„')
        group_stats: List[Dict[str, Any]] = []

        for group, subdf in df.groupby('åˆ†ç»„', sort=True):
            # Skip 'æœªåˆ†ç»„' for statistics and per-group image generation
            if str(group) == 'æœªåˆ†ç»„':
                continue
            # Ensure per-group sorting by å·®å€¼é™åº
            view = subdf[['æˆå‘˜', 'æˆ˜åŠŸæ€»é‡å·®å€¼']].sort_values('æˆ˜åŠŸæ€»é‡å·®å€¼', ascending=False).reset_index(drop=True)

            # Figure size heuristic based on rows; allocate top padding for title
            rows = len(view) + 1  # + header
            cols = 2
            cell_h = 0.42
            cell_w = 2.8
            top_pad_frac = 0.18  # more space for two-line title
            fig_h = max(3.5, rows * cell_h)
            fig_w = max(6.0, cols * cell_w)
            fig, ax = plt.subplots(figsize=(fig_w, fig_h))
            ax.axis('off')

            # Place table within bbox leaving space on top for the title
            the_table = ax.table(cellText=[[str(x) for x in row] for row in view.values],
                                  colLabels=list(view.columns),
                                  cellLoc='center',
                                  loc='center',
                                  bbox=[0.0, 0.0, 1.0, 1.0 - top_pad_frac])
            the_table.auto_set_font_size(False)
            the_table.set_fontsize(10)
            the_table.scale(1, 1.15)

            # Title above table region - multi-line
            title_text = f"{display_title}\n{group} ç»„ ({len(view)})"
            ax.text(0.5, 1.0 - top_pad_frac/2, title_text,
                    ha='center', va='center', transform=ax.transAxes, fontsize=12, fontweight='bold', linespacing=1.3)

            # Highlight by threshold: æˆ˜åŠŸæ€»é‡å·®å€¼ > high_delta_threshold
            try:
                high_rows = view.index[view['æˆ˜åŠŸæ€»é‡å·®å€¼'] > int(high_delta_threshold)].tolist()
                for i in high_rows:
                    r = i + 1  # offset for header row
                    for c in range(cols):
                        cell = the_table[(r, c)]
                        # Only change background color; keep borders same as non-highlighted cells
                        cell.set_facecolor('#FFF4CC')
            except Exception:
                pass

            # Additional highlight for zero-delta members using another color
            try:
                zero_rows = view.index[view['æˆ˜åŠŸæ€»é‡å·®å€¼'] == 0].tolist()
                for i in zero_rows:
                    r = i + 1  # offset for header row
                    for c in range(cols):
                        cell = the_table[(r, c)]
                        cell.set_facecolor('#E6F7FF')  # light blue for zero change
            except Exception:
                pass

            safe_group = str(group).replace('/', '_').replace('\\', '_')
            out_path = os.path.join(out_dir, f"{title_prefix}_åˆ†ç»„_{safe_group}.png")
            plt.savefig(out_path, bbox_inches='tight', dpi=200)
            plt.close(fig)
            saved_paths.append(out_path)

            # Collect aggregation metrics
            total_count = len(view)
            avg_delta = float(view['æˆ˜åŠŸæ€»é‡å·®å€¼'].mean()) if total_count else 0.0
            zero_count = int((view['æˆ˜åŠŸæ€»é‡å·®å€¼'] == 0).sum())
            group_stats.append({
                'åˆ†ç»„åç§°': group,
                'æœ‰æ•ˆæˆå‘˜äººæ•°': total_count,
                'å¹³å‡æˆ˜åŠŸå·®å€¼': round(avg_delta, 2),
                'ç‹—æ··å­äººæ•°': zero_count
            })

        # Create aggregated stats image if any stats collected
        if group_stats:
            stats_df = pd.DataFrame(group_stats)
            # Sort by å¹³å‡æˆ˜åŠŸå·®å€¼ desc
            stats_df = stats_df.sort_values('å¹³å‡æˆ˜åŠŸå·®å€¼', ascending=False).reset_index(drop=True)

            rows = len(stats_df) + 1
            cols = len(stats_df.columns)
            cell_h = 0.42
            cell_w = 1.6
            top_pad_frac = 0.15
            fig_h = max(3.0, rows * cell_h)
            fig_w = max(8.0, cols * cell_w)
            fig, ax = plt.subplots(figsize=(fig_w, fig_h))
            ax.axis('off')
            table = ax.table(cellText=[[str(x) for x in row] for row in stats_df.values],
                             colLabels=list(stats_df.columns),
                             cellLoc='center',
                             loc='center',
                             bbox=[0.0, 0.0, 1.0, 1.0 - top_pad_frac])
            table.auto_set_font_size(False)
            table.set_fontsize(10)
            table.scale(1, 1.15)
            ax.text(0.5, 1.0 - top_pad_frac/2,
                    f"{display_title} åˆ†ç»„æ±‡æ€»",
                    ha='center', va='center', transform=ax.transAxes, fontsize=13, fontweight='bold')
            agg_path = os.path.join(out_dir, f"{title_prefix}_åˆ†ç»„ç»Ÿè®¡æ±‡æ€».png")
            plt.savefig(agg_path, bbox_inches='tight', dpi=200)
            plt.close(fig)
            saved_paths.append(agg_path)
        return saved_paths


def _auto_find_two_csvs_in_test_data(root: str) -> Tuple[str, str]:
    td = os.path.join(root, 'test_data')
    if not os.path.isdir(td):
        raise FileNotFoundError(f"æœªæ‰¾åˆ°ç›®å½•: {td}")
    files = [os.path.join(td, f) for f in os.listdir(td) if f.lower().endswith('.csv')]
    if len(files) < 2:
        raise FileNotFoundError("test_data ä¸­å°‘äºä¸¤ä¸ªCSVæ–‡ä»¶")
    # Prefer files with timestamp pattern; sort by parsed ts
    def ts_or_min(path: str) -> datetime:
        try:
            return FileAnalyzer._parse_cn_timestamp_from_filename(path)
        except Exception:
            return datetime.min
    files = sorted(files, key=ts_or_min)
    return files[-2], files[-1]


if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser(description='åŒç›Ÿæˆå‘˜æˆ˜åŠŸæ€»é‡å·®å€¼åˆ†æ')
    parser.add_argument('--file1', type=str, help='CSVæ–‡ä»¶1è·¯å¾„ï¼ˆå«ä¸­æ–‡æ—¶é—´æˆ³ï¼‰')
    parser.add_argument('--file2', type=str, help='CSVæ–‡ä»¶2è·¯å¾„ï¼ˆå«ä¸­æ–‡æ—¶é—´æˆ³ï¼‰')
    args = parser.parse_args()

    root = os.path.dirname(os.path.abspath(__file__))
    f1, f2 = (args.file1, args.file2) if (args.file1 and args.file2) else _auto_find_two_csvs_in_test_data(root)

    analyzer = FileAnalyzer()
    out = analyzer.analyze_battle_merit_change(f1, f2)
    if not out.get('success'):
        print(f"åˆ†æå¤±è´¥: {out.get('error')}")
        raise SystemExit(1)

    print(f"æ—¶é—´èŒƒå›´: {out['earlier_ts']} -> {out['later_ts']}")
    print(f"æ–‡ä»¶é¡ºåº: æ—©={os.path.basename(out['earlier'])} æ™š={os.path.basename(out['later'])}")
    print("ç»“æœï¼ˆä»…ä¿ç•™ä¸¤è¾¹åŒæ—¶å­˜åœ¨çš„æˆå‘˜ï¼‰")
    print("ç»“æœï¼ˆæˆå‘˜, æˆ˜åŠŸæ€»é‡å·®å€¼, åˆ†ç»„ï¼‰ï¼ŒæŒ‰åˆ†ç»„ä¸å·®å€¼æ’åºï¼š")
    for row in out['rows']:
        print(f"{row['æˆå‘˜']}, {row['æˆ˜åŠŸæ€»é‡å·®å€¼']}, {row['åˆ†ç»„']}")

    # Save grouped tables as images (truncate timestamps to minute resolution for title)
    def _trim_seconds(ts_str: str) -> str:
        parts = ts_str.strip().split(' ')
        if len(parts) == 2 and parts[1].count(':') == 2:
            date_part, time_part = parts
            hh_mm = ':'.join(time_part.split(':')[:2])
            return f"{date_part} {hh_mm}"
        return ts_str
    earlier_no_sec = _trim_seconds(out['earlier_ts'])
    later_no_sec = _trim_seconds(out['later_ts'])
    title_prefix = f"æˆ˜åŠŸç»Ÿè®¡_{earlier_no_sec.replace(':','').replace(' ','_')}_è‡³_{later_no_sec.replace(':','').replace(' ','_')}"
    # Display title with slash-style date (YYYY/MM/DD HH:MM) and without seconds
    def _slash_fmt(ts: str) -> str:
        parts = ts.split(' ')
        if len(parts) == 2:
            d, hm = parts
            d_parts = d.split('-')
            if len(d_parts) == 3:
                d = '/'.join(d_parts)  # YYYY/MM/DD
            return f"{d} {hm}"
        return ts
    display_title = f"æˆ˜åŠŸç»Ÿè®¡ { _slash_fmt(earlier_no_sec) } â†’ { _slash_fmt(later_no_sec) }"
    out_dir = os.path.join(root, 'output')
    # Allow override of high-delta threshold via env var (default 5000)
    high_th = int(os.environ.get('HIGH_DELTA_THRESHOLD', '5000'))
    pngs = FileAnalyzer.save_grouped_tables_as_images(out['rows'], out_dir, title_prefix, display_title, high_delta_threshold=high_th)
    print("è¡¨æ ¼å›¾ç‰‡å·²ç”Ÿæˆï¼š")
    for p in pngs:
        print(p)

    # Optionally send via WeChat Work if credentials and target set
    try:
        # Try import dotenv lazily; ignore if not available
        try:
            from dotenv import load_dotenv  # type: ignore
            load_dotenv()
        except Exception:
            pass

        corp_id = os.environ.get('WECHAT_CORP_ID', '')
        corp_secret = os.environ.get('WECHAT_CORP_SECRET', '')
        agent_id = os.environ.get('WECHAT_AGENT_ID', '')
        to_user = os.environ.get('WECHAT_TO_USER', '')

        if to_user and corp_id and corp_secret and agent_id:
            try:
                from wechat_api import WeChatWorkAPI
                api = WeChatWorkAPI(corp_id, corp_secret, agent_id)
                print(f"å¼€å§‹æ¨é€åˆ°ä¼ä¸šå¾®ä¿¡ï¼Œç›®æ ‡: {to_user}")
                for path in pngs:
                    up = api.upload_image(path)
                    if up.get('errcode') == 0 and up.get('media_id'):
                        res = api.send_image_message(to_user, up['media_id'])
                        print(f"å‘é€å›¾ç‰‡: {os.path.basename(path)} -> {res}")
                    else:
                        print(f"ä¸Šä¼ å¤±è´¥: {path} -> {up}")
            except Exception as e_send:
                print(f"ä¼ä¸šå¾®ä¿¡å‘é€å¤±è´¥: {e_send}")
        # Always generate a manifest regardless of sending outcome
        manifest = {
            'title': title_prefix,
            'images': pngs,
            'wecom_push': {
                'corp_id_present': bool(corp_id),
                'agent_id_present': bool(agent_id),
                'to_user_present': bool(to_user)
            },
            'usage': 'è®¾ç½®ç¯å¢ƒå˜é‡ WECHAT_CORP_ID, WECHAT_CORP_SECRET, WECHAT_AGENT_ID, WECHAT_TO_USER å¯è‡ªåŠ¨æ¨é€ï¼›å¦åˆ™å¯æ‰‹åŠ¨ä½¿ç”¨ wechat_api å‘é€ã€‚'
        }
        with open(os.path.join(out_dir, f"{title_prefix}_wecom_manifest.json"), 'w', encoding='utf-8') as f:
            json.dump(manifest, f, ensure_ascii=False, indent=2)
        print("å·²ç”ŸæˆWeComæ¶ˆæ¯æ¸…å•JSONï¼ˆåŒ…å«å‘é€é…ç½®æç¤ºï¼‰ã€‚")
    except Exception as e:
        print(f"ä¼ä¸šå¾®ä¿¡æ¨é€æ­¥éª¤è·³è¿‡/å¤±è´¥: {e}")

